# LLM Compression Evidence Framework (大模型压缩证据框架)

> **关于本项目**：这是我整理的一份"大模型压缩/高效化"研究工作的**可审查证据快照**。我将原本复杂的模型压缩实验，转化为一套"证据驱动"的验证流程，旨在解决大模型研发中常见的"指标不可复现"和"口径漂移"痛点。
>
> ⚠️ **注意**：本仓库包含脱敏后的证据样例、数据契约 (Schema) 及自检脚本，不包含模型权重文件。

---

## 🚀 核心成果概览

我通过 V300/V400 系列算法，在 70B 大模型上实现了**超越主流方案（AWQ/GPTQ）的压缩效果**，并解锁了边缘设备的部署能力。

### 📊 核心竞争力对比

| 核心指标 | 本方案 (HoloForge) | 主流方案 (AWQ/GPTQ) | 传统量化 (RTN) |
|:---|:---|:---|:---|
| **70B模型支持** | ✅ **SOTA (5.8×)** | ⚠️ 有限 (3.2-4.0×) | ❌ 精度崩溃 |
| **精度保持** | ✅ **>0.999 Cosine** | 0.995-0.998 | <0.990 |
| **运行时内存** | ✅ **444MiB 增量** | 全量加载 | 全量加载 |
| **边缘部署** | ✅ **4GB设备可用** | 需24GB+显存 | 需24GB+显存 |
| **验证机制** | ✅ **Fail-Closed自检** | 人工验证 | 无 |

### 🔧 关键技术突破

#### 1. V300 Vajra-Precision 混合量化
*   **痛点解决**：解决高倍率压缩下的精度崩塌问题。
*   **核心方案**：基于 Hessian 敏感度分析的智能分层策略，关键层保留 FP16，冗余层下探至 INT4/INT2。
*   **多模型验证**：算法在 **DeepSeek-R1-Distill-Llama-70B** 与 **Qwen-2.5-72B** 上均通过了 SOTA 门禁验证，证明了方案在不同模型架构下的泛化能力。
*   **质量保障**：集成 `cos_min > 0.999` 自动化熔断机制，确保交付质量。

#### 2. V400 Adaptive Resonance-X 内存优化
*   **痛点解决**：大模型无法在 4GB/8GB 内存的边缘设备上运行。
*   **核心方案**：通过 Windowed-loading 技术，将算子级内存峰值增量控制在 **444MiB**。
*   **实际价值**：存储成本降低 **82.8%**，推理实例规格下降 2 个等级。

---

## 📂 仓库结构与验证

本项目的设计初衷是**"可审计"**。你可以通过以下方式验证我的工作成果，而无需运行昂贵的大模型。

### 目录说明

*   `evidence/`: 脱敏后的结构化证据样例（JSON格式）。
    *   `v300_vajra_sample.json`: 70B模型 5.8x 压缩证据。
    *   `v400_resonance_sample.json`: 102.4x 极限压缩探索证据。
    *   `breakthrough_evidence_sample.json`: 确定性验证证据 (`determinism_delta=0`)。
*   `schemas/`: 证据的数据契约（JSON Schema），用于定义字段标准。
*   `scripts/`: 自动化验证脚本。

### 🛠️ 一键自检

你可以直接运行以下命令，校验所有证据样例是否符合 Schema 定义，验证数据完整性：

```bash
# 安装依赖
pip install -r requirements.txt

# 运行验证脚本
python scripts/validate_evidence.py
```

**预期输出**：所有证据样本通过验证 (Validation successful)，退出码为 0。

---

## 💡 设计哲学：为什么做这个？

在过往的研发经历中，我发现"单点指标好看"往往不等于"系统可交付"。因此，我构建了这套证据框架：

1.  **把"口径争议"变成字段**：显式记录 `bytes_*`（字节口径）和 `determinism_*`（漂移来源），减少沟通成本。
2.  **Fail-Closed 机制**：任何未达标的压缩结果（如 Cosine < 0.999）会被自动标记为失败，而非人工修饰。
3.  **可复现优先**：保证 `determinism_delta = 0.0`，确保每一次实验结果都是可追溯的。

---

> Author: Thomas Tan (陈铭)
> GitHub: [chenmoke2022-sys](https://github.com/chenmoke2022-sys)
